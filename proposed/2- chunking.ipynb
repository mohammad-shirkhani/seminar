{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CVM_-JMp-YNX",
    "outputId": "311dc7ce-5124-435b-9e5c-97ab55c13ea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "ğŸ—‚ Ù…Ø³ÛŒØ± Ù‡Ø¯Ù: /content/drive/MyDrive/HotpotQA_snapshot/all_docs.json\n",
      "\n",
      "ğŸ“ Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙØ§ÛŒÙ„: 49.75 MB\n",
      "\n",
      "ğŸ“¥ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ JSON ...\n",
      "âœ… Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ÙˆÙÙ‚.\n",
      "\n",
      "==== Ù¾ÛŒØ´â€ŒÙ†Ù…Ø§ÛŒØ´ Ø³Ø§Ø®ØªØ§Ø± Ú©Ù„ÛŒ ====\n",
      "- Ù†ÙˆØ¹: dict | ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ÛŒØ¯Ù‡Ø§: 2\n",
      "  Ú©Ù„ÛŒØ¯ Ù†Ù…ÙˆÙ†Ù‡: 'titles' -> Ù†ÙˆØ¹ Ù…Ù‚Ø¯Ø§Ø±: list\n",
      "    - Ù†ÙˆØ¹: list | Ø·ÙˆÙ„: 4482\n",
      "      [#0] Ù†ÙˆØ¹ Ø¹Ø¶Ùˆ: str\n",
      "        - Ù…Ù‚Ø¯Ø§Ø± Ø³Ø§Ø¯Ù‡ (str): Anthony Avent\n",
      "      [#1] Ù†ÙˆØ¹ Ø¹Ø¶Ùˆ: str\n",
      "        - Ù…Ù‚Ø¯Ø§Ø± Ø³Ø§Ø¯Ù‡ (str): Newark, New Jersey\n",
      "      [#2] Ù†ÙˆØ¹ Ø¹Ø¶Ùˆ: str\n",
      "        - Ù…Ù‚Ø¯Ø§Ø± Ø³Ø§Ø¯Ù‡ (str): DraÅ¾en DalipagiÄ‡\n",
      "      ... (4479 Ø¹Ø¶Ùˆ Ø¯ÛŒÚ¯Ø±)\n",
      "  Ú©Ù„ÛŒØ¯ Ù†Ù…ÙˆÙ†Ù‡: 'docs' -> Ù†ÙˆØ¹ Ù…Ù‚Ø¯Ø§Ø±: list\n",
      "    - Ù†ÙˆØ¹: list | Ø·ÙˆÙ„: 4482\n",
      "      [#0] Ù†ÙˆØ¹ Ø¹Ø¶Ùˆ: str\n",
      "        - Ù…Ù‚Ø¯Ø§Ø± Ø³Ø§Ø¯Ù‡ (str): Anthony Avent (born October 18, 1969) is an American former professional bask...\n",
      "      [#1] Ù†ÙˆØ¹ Ø¹Ø¶Ùˆ: str\n",
      "        - Ù…Ù‚Ø¯Ø§Ø± Ø³Ø§Ø¯Ù‡ (str): Newark (/ËˆnjuËÉ™rk/ NEW-É™rk, locally [nÊŠÉ™É¹k]) is the most populous city in the...\n",
      "      [#2] Ù†ÙˆØ¹ Ø¹Ø¶Ùˆ: str\n",
      "        - Ù…Ù‚Ø¯Ø§Ø± Ø³Ø§Ø¯Ù‡ (str): DrazÌŒen \"Praja\" DalipagicÌ (Serbian Cyrillic: Ğ”Ñ€Ğ°Ğ¶ĞµĞ½ \"ĞŸÑ€Ğ°Ñ˜Ğ°\" Ğ”Ğ°Ğ»Ğ¸Ğ¿Ğ°Ğ³Ğ¸Ñ›; born ...\n",
      "      ... (4479 Ø¹Ø¶Ùˆ Ø¯ÛŒÚ¯Ø±)\n",
      "\n",
      "ğŸ” Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø§ Ø³Ø§Ø®ØªØ§Ø± ØºÛŒØ±Ù…ØªØ¯Ø§ÙˆÙ„ / ØªØ±Ú©ÛŒØ¨ÛŒ. Ø®Ø±ÙˆØ¬ÛŒ Ù¾ÛŒØ´â€ŒÙ†Ù…Ø§ÛŒØ´ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.\n",
      "\n",
      "âœ… ØªØ­Ù„ÛŒÙ„ Ø§ÙˆÙ„ÛŒÙ‡ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.\n",
      "\n",
      "ğŸ“Œ Ø®Ù„Ø§ØµÙ‡ Ø³Ø±ÛŒØ¹: Ù†ÙˆØ¹ Ø±ÛŒØ´Ù‡: dict | ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ÛŒØ¯: 2 | Ú†Ù†Ø¯ Ú©Ù„ÛŒØ¯ Ù†Ù…ÙˆÙ†Ù‡: ['titles', 'docs']\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import json, os, math, sys\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "FOLDER_NAME = \"HotpotQA_snapshot\"          \n",
    "FILENAME    = \"all_docs.json\"              \n",
    "BASE_PATH   = \"/content/drive/MyDrive\"     \n",
    "FILE_PATH   = os.path.join(BASE_PATH, FOLDER_NAME, FILENAME)\n",
    "\n",
    "SEARCH_IF_NOT_FOUND = True\n",
    "MAX_PREVIEW_ITEMS   = 3    \n",
    "# ==============================================\n",
    "\n",
    "print(\"ğŸ—‚ Ù…Ø³ÛŒØ± Ù‡Ø¯Ù:\", FILE_PATH)\n",
    "\n",
    "def search_file(filename, root=\"/content/drive\"):\n",
    "    matches = []\n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        if filename in filenames:\n",
    "            matches.append(os.path.join(dirpath, filename))\n",
    "    return matches\n",
    "\n",
    "if not os.path.isfile(FILE_PATH):\n",
    "    if SEARCH_IF_NOT_FOUND:\n",
    "        print(\"âš ï¸ ÙØ§ÛŒÙ„ Ø¯Ø± Ù…Ø³ÛŒØ± Ù…Ø³ØªÙ‚ÛŒÙ… Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯Ø› Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø³ØªØ¬Ùˆ...\")\n",
    "        candidates = search_file(FILENAME, BASE_PATH)\n",
    "        if not candidates:\n",
    "            raise FileNotFoundError(f\"ÙØ§ÛŒÙ„ {FILENAME} Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯. Ù…Ø³ÛŒØ± Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.\")\n",
    "        prioritized = [p for p in candidates if FOLDER_NAME in p]\n",
    "        chosen = prioritized[0] if prioritized else candidates[0]\n",
    "        print(\"Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡:\")\n",
    "        for i, c in enumerate(candidates, 1):\n",
    "            print(f\"  {i}. {c}\")\n",
    "        print(f\"âœ… Ù…Ø³ÛŒØ± Ø§Ù†ØªØ®Ø§Ø¨â€ŒØ´Ø¯Ù‡: {chosen}\")\n",
    "        FILE_PATH = chosen\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"ÙØ§ÛŒÙ„ Ø¯Ø± Ù…Ø³ÛŒØ± {FILE_PATH} Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.\")\n",
    "\n",
    "file_size_mb = os.path.getsize(FILE_PATH) / (1024*1024)\n",
    "print(f\"\\nğŸ“ Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙØ§ÛŒÙ„: {file_size_mb:.2f} MB\")\n",
    "\n",
    "print(\"\\nğŸ“¥ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ JSON ...\")\n",
    "with open(FILE_PATH, \"rb\") as raw:\n",
    "    raw_bytes = raw.read()\n",
    "\n",
    "BOM_UTF8 = b'\\xef\\xbb\\xbf'\n",
    "if raw_bytes.startswith(BOM_UTF8):\n",
    "    print(\"â„¹ï¸ BOM UTF-8 Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ø› Ø­Ø°Ù Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø¯.\")\n",
    "    raw_bytes = raw_bytes[len(BOM_UTF8):]\n",
    "\n",
    "text_data = raw_bytes.decode(\"utf-8\")\n",
    "\n",
    "data = json.loads(text_data)\n",
    "print(\"âœ… Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ÙˆÙÙ‚.\")\n",
    "\n",
    "def preview_structure(obj, depth=0, max_items=MAX_PREVIEW_ITEMS):\n",
    "    prefix = \"  \" * depth\n",
    "    if isinstance(obj, list):\n",
    "        print(f\"{prefix}- Ù†ÙˆØ¹: list | Ø·ÙˆÙ„: {len(obj)}\")\n",
    "        for i, item in enumerate(obj[:max_items]):\n",
    "            print(f\"{prefix}  [#{i}] Ù†ÙˆØ¹ Ø¹Ø¶Ùˆ: {type(item).__name__}\")\n",
    "            preview_structure(item, depth+2, max_items)\n",
    "        if len(obj) > max_items:\n",
    "            print(f\"{prefix}  ... ({len(obj) - max_items} Ø¹Ø¶Ùˆ Ø¯ÛŒÚ¯Ø±)\")\n",
    "    elif isinstance(obj, dict):\n",
    "        print(f\"{prefix}- Ù†ÙˆØ¹: dict | ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ÛŒØ¯Ù‡Ø§: {len(obj)}\")\n",
    "        for i, (k, v) in enumerate(list(obj.items())[:max_items]):\n",
    "            print(f\"{prefix}  Ú©Ù„ÛŒØ¯ Ù†Ù…ÙˆÙ†Ù‡: {repr(k)} -> Ù†ÙˆØ¹ Ù…Ù‚Ø¯Ø§Ø±: {type(v).__name__}\")\n",
    "            preview_structure(v, depth+2, max_items)\n",
    "        extra = len(obj) - max_items\n",
    "        if extra > 0:\n",
    "            print(f\"{prefix}  ... ({extra} Ú©Ù„ÛŒØ¯ Ø¯ÛŒÚ¯Ø±)\")\n",
    "    else:\n",
    "        s = str(obj)\n",
    "        if len(s) > 80: s = s[:77] + \"...\"\n",
    "        print(f\"{prefix}- Ù…Ù‚Ø¯Ø§Ø± Ø³Ø§Ø¯Ù‡ ({type(obj).__name__}): {s}\")\n",
    "\n",
    "def summarize_records(records):\n",
    "    df = pd.DataFrame(records)\n",
    "    print(\"\\nØ³ØªÙˆÙ†â€ŒÙ‡Ø§:\", list(df.columns))\n",
    "    print(\"ØªØ¹Ø¯Ø§Ø¯ Ø±Ú©ÙˆØ±Ø¯:\", len(df))\n",
    "    display(df.head(5))\n",
    "    stats = {}\n",
    "    if \"title\" in df.columns:\n",
    "        stats[\"unique_titles\"] = df[\"title\"].nunique()\n",
    "        stats[\"avg_title_len\"] = df[\"title\"].astype(str).str.len().mean()\n",
    "    text_col = next((c for c in [\"text\", \"content\", \"body\"] if c in df.columns), None)\n",
    "    if text_col:\n",
    "        lengths = df[text_col].astype(str).str.len()\n",
    "        stats.update({\n",
    "            \"text_col\": text_col,\n",
    "            \"avg_text_length\": lengths.mean(),\n",
    "            \"median_text_length\": lengths.median(),\n",
    "            \"min_text_length\": lengths.min(),\n",
    "            \"max_text_length\": lengths.max(),\n",
    "        })\n",
    "    print(\"\\nØ¢Ù…Ø§Ø± Ù…ØªÙ†ÛŒ:\")\n",
    "    for k, v in stats.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"\\n==== Ù¾ÛŒØ´â€ŒÙ†Ù…Ø§ÛŒØ´ Ø³Ø§Ø®ØªØ§Ø± Ú©Ù„ÛŒ ====\")\n",
    "preview_structure(data)\n",
    "\n",
    "if isinstance(data, list):\n",
    "    if data and all(isinstance(x, dict) for x in data):\n",
    "        print(\"\\nğŸ” ØªØ´Ø®ÛŒØµ: Ù„ÛŒØ³ØªÛŒ Ø§Ø² Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒâ€ŒÙ‡Ø§ (records).\")\n",
    "        summarize_records(data)\n",
    "    elif data and all(isinstance(x, str) for x in data):\n",
    "        print(\"\\nğŸ” ØªØ´Ø®ÛŒØµ: Ù„ÛŒØ³ØªÛŒ Ø§Ø² Ø±Ø´ØªÙ‡â€ŒÙ‡Ø§.\")\n",
    "        print(\"ØªØ¹Ø¯Ø§Ø¯ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§:\", len(data))\n",
    "        print(\"Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§:\", data[:5])\n",
    "    else:\n",
    "        print(\"\\nğŸ” Ø³Ø§Ø®ØªØ§Ø± Ù„ÛŒØ³Øª ØªØ±Ú©ÛŒØ¨ÛŒ/Ù…ØªÙØ§ÙˆØª Ø§Ø³ØªØ› Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¨ÛŒØ´ØªØ± Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.\")\n",
    "elif isinstance(data, dict):\n",
    "    if data and all(isinstance(v, str) for v in data.values()):\n",
    "        print(\"\\nğŸ” ØªØ´Ø®ÛŒØµ: Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ù†Ú¯Ø§Ø´Øª title -> text.\")\n",
    "        print(\"ØªØ¹Ø¯Ø§Ø¯ Ø±Ú©ÙˆØ±Ø¯:\", len(data))\n",
    "        for k, v in list(data.items())[:5]:\n",
    "            print(f\"- Ø¹Ù†ÙˆØ§Ù†: {k!r} | Ø·ÙˆÙ„ Ù…ØªÙ†: {len(v)}\")\n",
    "    elif any(isinstance(v, list) and v and isinstance(v[0], dict) for v in data.values()):\n",
    "        print(\"\\nğŸ” ØªØ´Ø®ÛŒØµ: Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø³Ø·Ø­ Ø¨Ø§Ù„Ø§ Ø¨Ø§ ÛŒÚ© ÛŒØ§ Ú†Ù†Ø¯ Ù„ÛŒØ³Øª Ø±Ú©ÙˆØ±Ø¯.\")\n",
    "        for k, v in data.items():\n",
    "            if isinstance(v, list) and v and isinstance(v[0], dict):\n",
    "                print(f\"\\n> ØªØ­Ù„ÛŒÙ„ Ù„ÛŒØ³Øª Ø±Ú©ÙˆØ±Ø¯ Ø²ÛŒØ± Ú©Ù„ÛŒØ¯: {k}\")\n",
    "                summarize_records(v)\n",
    "    else:\n",
    "        print(\"\\nğŸ” Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø§ Ø³Ø§Ø®ØªØ§Ø± ØºÛŒØ±Ù…ØªØ¯Ø§ÙˆÙ„ / ØªØ±Ú©ÛŒØ¨ÛŒ. Ø®Ø±ÙˆØ¬ÛŒ Ù¾ÛŒØ´â€ŒÙ†Ù…Ø§ÛŒØ´ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.\")\n",
    "else:\n",
    "    print(\"\\nğŸ” Ù…Ù‚Ø¯Ø§Ø± Ø³Ø§Ø¯Ù‡â€ŒÛŒ JSON (Ù†Ù‡ Ù„ÛŒØ³Øª Ùˆ Ù†Ù‡ dict):\", type(data).__name__)\n",
    "\n",
    "print(\"\\nâœ… ØªØ­Ù„ÛŒÙ„ Ø§ÙˆÙ„ÛŒÙ‡ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.\")\n",
    "\n",
    "def quick_summary(obj):\n",
    "    if isinstance(obj, list):\n",
    "        kind = \"list\"\n",
    "        length = len(obj)\n",
    "        elem_types = Counter(type(x).__name__ for x in obj[:100])\n",
    "        return f\"Ù†ÙˆØ¹ Ø±ÛŒØ´Ù‡: list | Ø·ÙˆÙ„ Ú©Ù„: {length} | Ø§Ù†ÙˆØ§Ø¹ 100 Ø¹Ø¶Ùˆ Ø§ÙˆÙ„: {dict(elem_types)}\"\n",
    "    if isinstance(obj, dict):\n",
    "        key_count = len(obj)\n",
    "        sample_keys = list(obj.keys())[:5]\n",
    "        return f\"Ù†ÙˆØ¹ Ø±ÛŒØ´Ù‡: dict | ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ÛŒØ¯: {key_count} | Ú†Ù†Ø¯ Ú©Ù„ÛŒØ¯ Ù†Ù…ÙˆÙ†Ù‡: {sample_keys}\"\n",
    "    return f\"Ø±ÛŒØ´Ù‡ Ù†ÙˆØ¹ Ø³Ø§Ø¯Ù‡: {type(obj).__name__}\"\n",
    "\n",
    "print(\"\\nğŸ“Œ Ø®Ù„Ø§ØµÙ‡ Ø³Ø±ÛŒØ¹:\", quick_summary(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dhk7YZJXPZ3Y",
    "outputId": "6e920bdc-0349-4a0f-e598-40d816d1e028"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Ø¯Ø± Ø­Ø§Ù„ Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ ÙˆØ±ÙˆØ¯ÛŒ: /content/drive/MyDrive/HotpotQA_snapshot/all_docs.json\n",
      "âœ… ØªØ¹Ø¯Ø§Ø¯ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ: 4482\n",
      "ğŸ”§ Ø¯Ø± Ø­Ø§Ù„ ØªÙˆÙ„ÛŒØ¯ Ù‚Ø·Ø¹Ø§Øª ... (Ù…Ù…Ú©Ù† Ø§Ø³Øª Ú©Ù…ÛŒ Ø²Ù…Ø§Ù† Ø¨Ø¨Ø±Ø¯)\n",
      "âœ… ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù‚Ø·Ø¹Ø§Øª: 66237\n",
      "ğŸ’¾ ÙØ§ÛŒÙ„ JSON (Ø¢Ø±Ø§ÛŒÙ‡â€ŒØ§ÛŒ) Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: /content/drive/MyDrive/HotpotQA_snapshot/all_docs_chunks.json\n",
      "ğŸ“ Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙØ§ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ: 64.99 MB\n",
      "\n",
      "ğŸ” Ù¾ÛŒØ´â€ŒÙ†Ù…Ø§ÛŒØ´ 3 Ø±Ú©ÙˆØ±Ø¯ Ø§ÙˆÙ„:\n",
      "------------------------------------------------------------\n",
      "id: 0\n",
      "title_chunk: Anthony Avent\n",
      "chunk_index: 1 / 2\n",
      "original_index: 0\n",
      "doc_chunk (Ú©Ù„Ù…Ø§Øª ~150):\n",
      "Anthony Avent (born October 18, 1969) is an American former professional basketball player who was selected by the Atlanta Hawks in the first round (15th pick overall) of the 1991 NBA draft. Born in Rocky Mount, North Carolina, Avent played for the Milwaukee Bucks, Orlando Magic, Vancouver Grizzlies, Utah Jazz and Los Angeles Clippers in six NBA seasons. He played collegiately at Seton Hall Univer...\n",
      "------------------------------------------------------------\n",
      "id: 1\n",
      "title_chunk: Anthony Avent\n",
      "chunk_index: 2 / 2\n",
      "original_index: 0\n",
      "doc_chunk (Ú©Ù„Ù…Ø§Øª ~71):\n",
      "signed a four-year deal with the Bucks, beginning with a $500,000 installment in his first season and increasing in $150,000 increments in each of the succeeding three seasons. Thus, Avent would make $950,000 in the fourth year of his contract. His average salary would be $725,000 per season.In the 1996â€“97 season he played in several games for the perennially powerful Greek team Panathinaikos, and...\n",
      "------------------------------------------------------------\n",
      "id: 2\n",
      "title_chunk: Newark, New Jersey\n",
      "chunk_index: 1 / 95\n",
      "original_index: 1\n",
      "doc_chunk (Ú©Ù„Ù…Ø§Øª ~150):\n",
      "Newark (/ËˆnjuËÉ™rk/ NEW-É™rk, locally [nÊŠÉ™É¹k]) is the most populous city in the U.S. state of New Jersey and the seat of Essex County. As of the 2020 census, the city's population was 311,549, an increase of 34,409 (+12.4%) from the 2010 census count of 277,140, which in turn reflected an increase of 3,594 (+1.3%) from the 273,546 counted in the 2000 census. The Population Estimates Program calculat...\n",
      "\n",
      "âœ… Ú©Ø§Ø± ØªÙ…Ø§Ù… Ø´Ø¯.\n"
     ]
    }
   ],
   "source": [
    "# Chunking HotpotQA all_docs.json  \n",
    "\n",
    "import os, json\n",
    "from typing import List\n",
    "\n",
    "FOLDER_NAME           = \"HotpotQA_snapshot\"\n",
    "INPUT_FILENAME        = \"all_docs.json\"\n",
    "OUTPUT_FILENAME       = \"all_docs_chunks.json\"      \n",
    "OUTPUT_JSONL_FILENAME = \"all_docs_chunks.jsonl\"     \n",
    "BASE_PATH             = \"/content/drive/MyDrive\"\n",
    "\n",
    "CHUNK_SIZE_WORDS      = 150\n",
    "CHUNK_OVERLAP_WORDS   = 20\n",
    "PRINT_SAMPLE          = 3\n",
    "WRITE_JSON_ARRAY      = True    \n",
    "WRITE_JSONL           = False   \n",
    "# -----------------------------------------\n",
    "\n",
    "INPUT_PATH        = os.path.join(BASE_PATH, FOLDER_NAME, INPUT_FILENAME)\n",
    "OUTPUT_PATH       = os.path.join(BASE_PATH, FOLDER_NAME, OUTPUT_FILENAME)\n",
    "OUTPUT_JSONL_PATH = os.path.join(BASE_PATH, FOLDER_NAME, OUTPUT_JSONL_FILENAME)\n",
    "\n",
    "print(f\"ğŸ“¥ Ø¯Ø± Ø­Ø§Ù„ Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ ÙˆØ±ÙˆØ¯ÛŒ: {INPUT_PATH}\")\n",
    "\n",
    "if not os.path.isfile(INPUT_PATH):\n",
    "    raise FileNotFoundError(\"ÙØ§ÛŒÙ„ ÙˆØ±ÙˆØ¯ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.\")\n",
    "\n",
    "with open(INPUT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw = json.load(f)\n",
    "\n",
    "titles = raw.get(\"titles\")\n",
    "docs   = raw.get(\"docs\")\n",
    "\n",
    "if not (isinstance(titles, list) and isinstance(docs, list) and len(titles) == len(docs)):\n",
    "    raise ValueError(\"Ø³Ø§Ø®ØªØ§Ø± ÙØ§ÛŒÙ„ Ù…Ø·Ø§Ø¨Ù‚ Ø§Ù†ØªØ¸Ø§Ø± Ù†ÛŒØ³Øª (Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§ÛŒ titles Ùˆ docs Ø¨Ø§ Ø·ÙˆÙ„ Ø¨Ø±Ø§Ø¨Ø±).\")\n",
    "\n",
    "n_records = len(titles)\n",
    "print(f\"âœ… ØªØ¹Ø¯Ø§Ø¯ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ: {n_records}\")\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int, overlap: int) -> List[str]:\n",
    "    words = text.split()\n",
    "    n = len(words)\n",
    "    if n == 0:\n",
    "        return []\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < n:\n",
    "        end = start + chunk_size\n",
    "        chunks.append(\" \".join(words[start:end]))\n",
    "        if end >= n:\n",
    "            break\n",
    "        start = end - overlap\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "    return chunks\n",
    "\n",
    "if WRITE_JSONL:\n",
    "    os.makedirs(os.path.dirname(OUTPUT_JSONL_PATH), exist_ok=True)\n",
    "    jsonl_f = open(OUTPUT_JSONL_PATH, \"w\", encoding=\"utf-8\")\n",
    "else:\n",
    "    jsonl_f = None\n",
    "\n",
    "all_chunk_records = [] \n",
    "global_id = 0\n",
    "\n",
    "print(\"ğŸ”§ Ø¯Ø± Ø­Ø§Ù„ ØªÙˆÙ„ÛŒØ¯ Ù‚Ø·Ø¹Ø§Øª ... (Ù…Ù…Ú©Ù† Ø§Ø³Øª Ú©Ù…ÛŒ Ø²Ù…Ø§Ù† Ø¨Ø¨Ø±Ø¯)\")\n",
    "\n",
    "for original_index, (title, doc) in enumerate(zip(titles, docs)):\n",
    "    chunks = chunk_text(doc, CHUNK_SIZE_WORDS, CHUNK_OVERLAP_WORDS)\n",
    "    total_chunks = len(chunks)\n",
    "    for ci, ch in enumerate(chunks):\n",
    "        rec = {\n",
    "            \"id\": global_id,            # Ø´Ù†Ø§Ø³Ù‡ ÛŒÚ©ØªØ§\n",
    "            \"title_chunk\": title,       # Ø·Ø¨Ù‚ Ø¯Ø±Ø®ÙˆØ§Ø³Øª\n",
    "            \"doc_chunk\": ch,\n",
    "            \"chunk_index\": ci,\n",
    "            \"total_chunks\": total_chunks,\n",
    "            \"original_index\": original_index\n",
    "        }\n",
    "        if WRITE_JSON_ARRAY:\n",
    "            all_chunk_records.append(rec)\n",
    "        if WRITE_JSONL:\n",
    "            jsonl_f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "        global_id += 1\n",
    "\n",
    "if WRITE_JSONL:\n",
    "    jsonl_f.close()\n",
    "    print(f\"ğŸ’¾ ÙØ§ÛŒÙ„ JSONLines Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {OUTPUT_JSONL_PATH}\")\n",
    "\n",
    "print(f\"âœ… ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù‚Ø·Ø¹Ø§Øª: {global_id}\")\n",
    "\n",
    "if WRITE_JSON_ARRAY:\n",
    "    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "    with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(all_chunk_records, f_out, ensure_ascii=False)\n",
    "    out_size_mb = os.path.getsize(OUTPUT_PATH) / (1024*1024)\n",
    "    print(f\"ğŸ’¾ ÙØ§ÛŒÙ„ JSON (Ø¢Ø±Ø§ÛŒÙ‡â€ŒØ§ÛŒ) Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {OUTPUT_PATH}\")\n",
    "    print(f\"ğŸ“ Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙØ§ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ: {out_size_mb:.2f} MB\")\n",
    "\n",
    "print(f\"\\nğŸ” Ù¾ÛŒØ´â€ŒÙ†Ù…Ø§ÛŒØ´ {PRINT_SAMPLE} Ø±Ú©ÙˆØ±Ø¯ Ø§ÙˆÙ„:\")\n",
    "preview_list = all_chunk_records[:PRINT_SAMPLE] if WRITE_JSON_ARRAY else []\n",
    "if not preview_list and WRITE_JSONL:\n",
    "    with open(OUTPUT_JSONL_PATH, \"r\", encoding=\"utf-8\") as f_preview:\n",
    "        for _ in range(PRINT_SAMPLE):\n",
    "            line = f_preview.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            preview_list.append(json.loads(line))\n",
    "\n",
    "for sample in preview_list:\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"id: {sample['id']}\")\n",
    "    print(f\"title_chunk: {sample['title_chunk']}\")\n",
    "    print(f\"chunk_index: {sample['chunk_index'] + 1} / {sample['total_chunks']}\")\n",
    "    print(f\"original_index: {sample['original_index']}\")\n",
    "    print(f\"doc_chunk (Ú©Ù„Ù…Ø§Øª ~{len(sample['doc_chunk'].split())}):\")\n",
    "    print(sample['doc_chunk'][:400] + (\"...\" if len(sample['doc_chunk']) > 400 else \"\"))\n",
    "\n",
    "print(\"\\nâœ… Ú©Ø§Ø± ØªÙ…Ø§Ù… Ø´Ø¯.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uy2NepVMQayo",
    "outputId": "df387de2-3ce8-42e8-9593-cad68a737dc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "id: 0\n",
      "title_chunk: Anthony Avent\n",
      "chunk_index: 1 / 2\n",
      "original_index: 0\n",
      "doc_chunk (Ú©Ù„Ù…Ø§Øª ~150):\n",
      "Anthony Avent (born October 18, 1969) is an American former professional basketball player who was selected by the Atlanta Hawks in the first round (15th pick overall) of the 1991 NBA draft. Born in Rocky Mount, North Carolina, Avent played for the Milwaukee Bucks, Orlando Magic, Vancouver Grizzlies, Utah Jazz and Los Angeles Clippers in six NBA seasons. He played collegiately at Seton Hall University where he played in the 1989 NCAA championship game. Prior to Seton Hall, Avent played at Malcolm X Shabazz High School in Newark, New Jersey.Upon being drafted 15th overall by the Bucks, Avent went on to instead sign with Phonola Caserta of the Italian League. He made this decision after failing to reach a satisfactory contract with the Bucks. After one season in Italy, Avent signed a four-year deal with the Bucks, beginning with a $500,000 installment in his first season and increasing in $150,000\n",
      "------------------------------------------------------------\n",
      "id: 1\n",
      "title_chunk: Anthony Avent\n",
      "chunk_index: 2 / 2\n",
      "original_index: 0\n",
      "doc_chunk (Ú©Ù„Ù…Ø§Øª ~71):\n",
      "signed a four-year deal with the Bucks, beginning with a $500,000 installment in his first season and increasing in $150,000 increments in each of the succeeding three seasons. Thus, Avent would make $950,000 in the fourth year of his contract. His average salary would be $725,000 per season.In the 1996â€“97 season he played in several games for the perennially powerful Greek team Panathinaikos, and in 2001 he played for PAOK BC.\n",
      "------------------------------------------------------------\n",
      "id: 2\n",
      "title_chunk: Newark, New Jersey\n",
      "chunk_index: 1 / 95\n",
      "original_index: 1\n",
      "doc_chunk (Ú©Ù„Ù…Ø§Øª ~150):\n",
      "Newark (/ËˆnjuËÉ™rk/ NEW-É™rk, locally [nÊŠÉ™É¹k]) is the most populous city in the U.S. state of New Jersey and the seat of Essex County. As of the 2020 census, the city's population was 311,549, an increase of 34,409 (+12.4%) from the 2010 census count of 277,140, which in turn reflected an increase of 3,594 (+1.3%) from the 273,546 counted in the 2000 census. The Population Estimates Program calculated a population of 305,344 for 2022, making it the 66th-most populous municipality in the nation.Settled in 1666 by Puritans from New Haven Colony, Newark is one of the oldest cities in the United States. Its location at the mouth of the Passaic River (where it flows into Newark Bay) has made the city's waterfront an integral part of the Port of New York and New Jersey. Today, Port Newarkâ€“Elizabeth is the primary container shipping terminal of the busiest seaport on the U.S. East\n",
      "\n",
      "âœ… Ú©Ø§Ø± ØªÙ…Ø§Ù… Ø´Ø¯.\n"
     ]
    }
   ],
   "source": [
    "for sample in preview_list:\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"id: {sample['id']}\")\n",
    "    print(f\"title_chunk: {sample['title_chunk']}\")\n",
    "    print(f\"chunk_index: {sample['chunk_index'] + 1} / {sample['total_chunks']}\")\n",
    "    print(f\"original_index: {sample['original_index']}\")\n",
    "    print(f\"doc_chunk (Ú©Ù„Ù…Ø§Øª ~{len(sample['doc_chunk'].split())}):\")\n",
    "    print(sample['doc_chunk'][:])\n",
    "\n",
    "print(\"\\nâœ… Ú©Ø§Ø± ØªÙ…Ø§Ù… Ø´Ø¯.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5iB0MzDDRBEd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
