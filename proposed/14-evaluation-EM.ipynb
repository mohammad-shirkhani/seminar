{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MQ-awlKKDpLD",
    "outputId": "8caefbde-290e-4b80-8ed8-87da8d4ea054"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HwxFE2xLETgS",
    "outputId": "d1df5574-b6d7-430c-ab54-571587ed7b3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q sentence-transformers tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4aelfPHqEUpZ",
    "outputId": "ba039a38-3b69-443b-bcb2-4ea629bed58c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File copied to /content/llm_eval/hotpotqa_answers_qwen.json\n"
     ]
    }
   ],
   "source": [
    "import json, shutil\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "drive_file = Path('/content/drive/MyDrive/HotpotQA_snapshot/hotpotqa_answers_qwen.json')\n",
    "assert drive_file.exists(), f\"{drive_file} not found.\"\n",
    "\n",
    "work_dir = Path('/content/llm_eval')\n",
    "work_dir.mkdir(parents=True, exist_ok=True)\n",
    "local_file = work_dir / drive_file.name\n",
    "shutil.copy2(drive_file, local_file)\n",
    "print('✅ File copied to', local_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QxMzLo7pEV-s"
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def evaluate_em(json_path, sim_threshold=0.9):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    model = SentenceTransformer(\n",
    "        \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\",\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    total = correct = 0\n",
    "    total_bridge = correct_bridge = 0\n",
    "    total_comp   = correct_comp   = 0\n",
    "\n",
    "    for rec in tqdm(data, desc=\"Evaluating\"):\n",
    "        ev_texts = rec.get('evidence', [])\n",
    "        if not ev_texts:          \n",
    "            continue\n",
    "\n",
    "        ev_embs = model.encode(ev_texts, device=DEVICE)\n",
    "\n",
    "        for support in rec.get('supports', []):\n",
    "            sent = support[1] if isinstance(support, list) else support\n",
    "            s_emb = model.encode(sent, device=DEVICE)\n",
    "            sims  = util.dot_score(s_emb, ev_embs).cpu().numpy().flatten()\n",
    "            hit   = sims.max() > sim_threshold   \n",
    "\n",
    "            total += 1\n",
    "            if hit:\n",
    "                correct += 1\n",
    "\n",
    "            if rec.get('type') == 'bridge':\n",
    "                total_bridge += 1\n",
    "                if hit: correct_bridge += 1\n",
    "            else:           \n",
    "                total_comp += 1\n",
    "                if hit: correct_comp += 1\n",
    "\n",
    "    return {\n",
    "        'EM_all':        correct / total if total else 0,\n",
    "        'EM_bridge':     correct_bridge / total_bridge if total_bridge else 0,\n",
    "        'EM_comparison': correct_comp / total_comp if total_comp else 0\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1HS9Sco1EXOC",
    "outputId": "238fb2b9-fbd4-4fd8-a857-8d1a800f517c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:03<00:00, 30.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 HotpotQA EM Scores:\n",
      "  • All supports:           0.4286\n",
      "  • Bridge‐type supports:   0.4234\n",
      "  • Comparison‐type supports: 0.4352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_em(local_file)\n",
    "\n",
    "print(\"\\n🔹 HotpotQA EM Scores:\")\n",
    "print(f\"  • All supports:           {results['EM_all']:.4f}\")\n",
    "print(f\"  • Bridge‐type supports:   {results['EM_bridge']:.4f}\")\n",
    "print(f\"  • Comparison‐type supports: {results['EM_comparison']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
