{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1lZlveqi4p_Q"
   },
   "outputs": [],
   "source": [
    "# 1  Install & import libraries\n",
    "!pip install -q google-genai tqdm\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from pathlib import Path\n",
    "import os, json, time, re, shutil, tqdm\n",
    "from typing import List, Dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aLxasarh8r9R",
    "outputId": "3570880e-64fd-4a52-d1f1-1c8846241470"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Gemini client initialised\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# 2  Authenticate Gemini & mount Drive\n",
    "\n",
    "# === Provide your API key (or export it beforehand) ===\n",
    "os.environ['GEMINI_API_KEY'] = ''  \n",
    "\n",
    "client = genai.Client()\n",
    "print('âœ… Gemini client initialised')\n",
    "\n",
    "# Mount Drive (optional â€“ comment out if running locally)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T2LYfcGm8tjn",
    "outputId": "26ad016c-fa05-4130-9c70-f180be55b326"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Evidence file copied to working dir\n"
     ]
    }
   ],
   "source": [
    "\n",
    "drive_root = Path('/content/drive/MyDrive/HotpotQA_snapshot')\n",
    "work_dir   = Path('/content/llm_answer_generation')\n",
    "work_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "src_file   = drive_root / 'evidence_hotpotqa_entities_with_paths.json'\n",
    "dst_file   = work_dir   / src_file.name\n",
    "\n",
    "shutil.copy2(src_file, dst_file)\n",
    "print('ðŸ“„ Evidence file copied to working dir')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "groWeqj-8vp-"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_json(path: Path) -> List[Dict]:\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(obj, path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(obj, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "def _clean_answer(text) -> str:\n",
    "    \"\"\"Return a whitespace-collapsed answer; tolerate None/empty inputs.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r\"<think[^>]*?>.*?</think>\", \" \", text, flags=re.S)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "def split_thoughts_and_answer(parts):\n",
    "    \"\"\"Separate thought summaries from the final answer (returns thoughts, answer).\"\"\"\n",
    "    thoughts, answer = [], []\n",
    "    for p in parts:\n",
    "        if not getattr(p, 'text', None):\n",
    "            continue\n",
    "        if getattr(p, 'thought', False):\n",
    "            thoughts.append(p.text)\n",
    "        else:\n",
    "            answer.append(p.text)\n",
    "    return \"\\n\".join(thoughts).strip(), \" \".join(answer).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YqBxN4gB8yDe"
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_answers(\n",
    "    evidence_file: Path,\n",
    "    save_file: Path,\n",
    "    *,\n",
    "    max_output_tokens: int = 128,\n",
    "    temperature: float = 0.6,\n",
    "    top_p: float = 0.95,\n",
    "    thinking_budget: int = -1,   # dynamic\n",
    "    pause_secs: float = 5.0,\n",
    "):\n",
    "    data = load_json(evidence_file)\n",
    "    responses = []\n",
    "\n",
    "    prompt_template = (\n",
    "        'Given the question, its associated knowledge-graph paths, and evidence contexts below, '\n",
    "        'please generate a concise, precise answer in English. The answer must strictly adhere to the following guidelines:\\n'\n",
    "        '- The answer should be directly relevant to the question.\\n'\n",
    "        '- Provide the answer in a clear, straightforward format.\\n'\n",
    "        '- Limit your answer to **no more than 10 words**, focusing on the essential information requested.\\n'\n",
    "        '- Use both the provided information and your own knowledge to answer as accurately as possible.\\n\\n'\n",
    "        'QUESTION: {question}\\n'\n",
    "        'PATHS:\\n{paths}\\n'\n",
    "        'CONTEXT:\\n{context}\\n'\n",
    "        'ANSWER:'\n",
    "    )\n",
    "\n",
    "    none_template = (\n",
    "        'Given the following question, create a concise (â‰¤10 words) English answer using your own knowledge if needed.\\n'\n",
    "        'QUESTION: {question}\\n'\n",
    "        'ANSWER:'\n",
    "    )\n",
    "\n",
    "    for rec in tqdm.tqdm(data, total=len(data)):\n",
    "        ctx_list  = rec.get('evidence', [])\n",
    "        paths_list = rec.get('Final_paths') or rec.get('paths', [])\n",
    "\n",
    "        if ctx_list or paths_list:\n",
    "            ctx_block   = \"\\n\".join(f\"{i+1}: {c}\" for i, c in enumerate(ctx_list))\n",
    "            paths_block = \"\\n\".join(f\"{i+1}: {p}\" for i, p in enumerate(paths_list))\n",
    "            user_prompt = prompt_template.format(\n",
    "                question = rec['question'],\n",
    "                paths    = paths_block if paths_block else 'N/A',\n",
    "                context  = ctx_block if ctx_block else 'N/A'\n",
    "            )\n",
    "        else:\n",
    "            user_prompt = none_template.format(question=rec['question'])\n",
    "\n",
    "        response = client.models.generate_content(\n",
    "            model='gemini-2.5-flash',\n",
    "            contents=user_prompt,\n",
    "            config=types.GenerateContentConfig(\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                max_output_tokens=max_output_tokens,\n",
    "                thinking_config=types.ThinkingConfig(\n",
    "                    thinking_budget=thinking_budget,\n",
    "                    include_thoughts=True\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            cand_container = response.candidates\n",
    "            cand = cand_container[0] if isinstance(cand_container, (list, tuple)) else cand_container\n",
    "            parts = cand.content.parts\n",
    "            thought_text, answer_text = split_thoughts_and_answer(parts)\n",
    "        except (TypeError, IndexError, AttributeError):\n",
    "            thought_text = \"\"\n",
    "            answer_text  = getattr(response, \"text\", \"Information not available\")\n",
    "\n",
    "        short_answer = _clean_answer(answer_text)\n",
    "\n",
    "        responses.append({\n",
    "            'type':         rec.get('type'),\n",
    "            'question':     rec['question'],\n",
    "            'answer':       rec.get('answer'),      # ground-truth answer untouched\n",
    "            'thoughts':     thought_text,\n",
    "            'response':     answer_text,            \n",
    "            'short_answer': short_answer,          \n",
    "        })\n",
    "\n",
    "        save_json(responses, save_file)  \n",
    "        time.sleep(pause_secs)           \n",
    "\n",
    "    print(f\"Finished â€“ saved to {save_file}\")\n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PPoVkvw58zsp",
    "outputId": "e65a9d0d-e6ad-4974-8ea5-c1880a0eb1b3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [24:31<00:00, 14.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished â€“ saved to /content/drive/MyDrive/HotpotQA_snapshot/hotpotqa_answers_gemini.json\n",
      "âœ… All done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 6  Run generation on HotpotQA\n",
    "\n",
    "hotpot_out = drive_root / 'hotpotqa_answers_gemini.json'\n",
    "\n",
    "_ = generate_answers(\n",
    "        evidence_file = work_dir / 'evidence_hotpotqa_entities_with_paths.json',\n",
    "        save_file     = hotpot_out,\n",
    "        max_output_tokens = 1024,\n",
    "        temperature       = 0.6,\n",
    "        top_p             = 0.95,\n",
    "        thinking_budget   = -1,   # dynamic\n",
    "        pause_secs        = 10,\n",
    ")\n",
    "\n",
    "print('âœ… All done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FlZEL0L81zu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
