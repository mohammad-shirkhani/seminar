{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qDEG65gA_Gax",
    "outputId": "26b80fbd-e78d-4c14-8957-23907dfee485"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA L4 (UUID: GPU-af3496cf-f50f-c72f-add3-6d87096c9e91)\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!nvidia-smi -L\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AP1cfyPy_Iv3"
   },
   "outputs": [],
   "source": [
    "#2\n",
    "!pip install -q networkx tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7_sYQOr-_KB-"
   },
   "outputs": [],
   "source": [
    "#3\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(\"/content/drive/MyDrive/HotpotQA_snapshot\")\n",
    "JSON_PATH      = BASE_DIR / \"all_docs_chunks_entities_relations_all.json\"\n",
    "\n",
    "ENT_ORDER_PATH = BASE_DIR / \"unique_entities_ordered.txt\"\n",
    "REL_ORDER_PATH = BASE_DIR / \"unique_relations_ordered.txt\"\n",
    "\n",
    "GRAPH_PKL_PATH = BASE_DIR / \"hotpotqa_kg_v2.gpickle\"    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "McKg8x___LXO",
    "outputId": "d1a3b8a3-cf98-4135-a2fe-bfec720c6d96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entitiesÂ : 557,825\n",
      "relations: 139,253\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "def load_ordered(path: Path) -> list[str]:\n",
    "    with path.open(encoding=\"utfâ€‘8\") as f:\n",
    "        return [ln.rstrip(\"\\n\") for ln in f if ln.strip()]\n",
    "\n",
    "ordered_entities  = load_ordered(ENT_ORDER_PATH)\n",
    "ordered_relations = load_ordered(REL_ORDER_PATH)\n",
    "\n",
    "entity2idx   = {txt: i for i, txt in enumerate(ordered_entities)}\n",
    "relation2idx = {txt: i for i, txt in enumerate(ordered_relations)}\n",
    "\n",
    "print(f\"entitiesÂ : {len(entity2idx):,}\")\n",
    "print(f\"relations: {len(relation2idx):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GuwdOUxd_Mn2"
   },
   "outputs": [],
   "source": [
    "#5\n",
    "import re, unicodedata, itertools\n",
    "_WS_RE       = re.compile(r\"\\s+\", re.UNICODE)\n",
    "QUOTE_CHARS  = '\"\\'â€œâ€â€˜â€™Â´â€›âââ®â¯â¹‚ã€ã€Â«Â»â€šâ€â€â€³â€²Ë'\n",
    "PUNCT_TAIL   = ',.;:!?â˜\\u2014\\u2013-'\n",
    "\n",
    "def _strip_outer_quotes(t: str) -> str:\n",
    "    return t[1:-1] if len(t) >= 2 and t[0] in QUOTE_CHARS and t[-1] in QUOTE_CHARS else t\n",
    "\n",
    "def normalize_ent(txt: str) -> str:\n",
    "    s = txt.strip()\n",
    "    while s and s[0] in QUOTE_CHARS: s = s[1:]\n",
    "    while s and s[-1] in QUOTE_CHARS: s = s[:-1]\n",
    "    s = _strip_outer_quotes(s).strip(PUNCT_TAIL + \" \")\n",
    "    s = unicodedata.normalize(\"NFKC\", s).casefold()\n",
    "    return _WS_RE.sub(\" \", s).strip()\n",
    "\n",
    "def clean_rel(middle: str) -> str:\n",
    "    r = middle.strip()\n",
    "    while r and r[0] in QUOTE_CHARS: r = r[1:]\n",
    "    while r and r[-1] in QUOTE_CHARS: r = r[:-1]\n",
    "    r = _strip_outer_quotes(r).strip(PUNCT_TAIL + \" \")\n",
    "    r = unicodedata.normalize(\"NFKC\", r).casefold()\n",
    "    r = _WS_RE.sub(\" \", r).strip()\n",
    "    return r.replace(\" \", \"_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QvNF7Xpp_Ny-",
    "outputId": "53b8b686-e761-4541-86d4-5d74f875e7ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³Â loading JSON â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scan: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66237/66237 [00:23<00:00, 2858.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â­•Â missing entities in lookup : 97263\n",
      "â­•Â missing relations in lookup: 4\n",
      "nodes collected : 557,821\n",
      "edges collected : 1,075,644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "import json, collections, tqdm\n",
    "\n",
    "print(\"â³Â loading JSON â€¦\")\n",
    "with JSON_PATH.open(encoding=\"utfâ€‘8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "NodeInfo = collections.namedtuple(\"NodeInfo\", \"types chunk_ids\")\n",
    "node_tmp  : dict[int, NodeInfo]          = {}\n",
    "edge_tmp  : dict[tuple[int,int,int], set[int]] = collections.defaultdict(set)\n",
    "\n",
    "missing_ent, missing_rel = 0, 0\n",
    "\n",
    "for ch in tqdm.tqdm(chunks, desc=\"scan\"):\n",
    "    cid   = ch[\"id\"]\n",
    "    ents  = ch.get(\"entities\", [])\n",
    "    rels  = ch.get(\"relations\", [])\n",
    "\n",
    "    for ent in ents:\n",
    "        e_norm = normalize_ent(ent[\"text\"])\n",
    "        idx    = entity2idx.get(e_norm)\n",
    "        if idx is None:          \n",
    "            missing_ent += 1\n",
    "            continue\n",
    "        ni = node_tmp.setdefault(idx, NodeInfo(set(), set()))\n",
    "        ni.types.add(ent[\"type\"])\n",
    "        ni.chunk_ids.add(cid)\n",
    "\n",
    "    for rel_raw in rels:\n",
    "        if rel_raw.count(\"->\") < 2:\n",
    "            continue\n",
    "        parts = [p.strip() for p in rel_raw.split(\"->\") if p.strip()]\n",
    "        head_norm, tail_norm = normalize_ent(parts[0]), normalize_ent(parts[-1])\n",
    "        middle_raw           = \"_\".join(parts[1:-1]) if len(parts) > 2 else parts[1]\n",
    "        rel_norm             = clean_rel(middle_raw)\n",
    "\n",
    "        h_idx = entity2idx.get(head_norm)\n",
    "        t_idx = entity2idx.get(tail_norm)\n",
    "        r_idx = relation2idx.get(rel_norm)\n",
    "\n",
    "        if None in (h_idx, t_idx):\n",
    "            missing_ent += 1\n",
    "            continue\n",
    "        if r_idx is None:\n",
    "            missing_rel += 1\n",
    "            continue\n",
    "\n",
    "        node_tmp.setdefault(h_idx, NodeInfo(set(), set())).chunk_ids.add(cid)\n",
    "        node_tmp.setdefault(t_idx, NodeInfo(set(), set())).chunk_ids.add(cid)\n",
    "\n",
    "        edge_tmp[(h_idx, r_idx, t_idx)].add(cid)\n",
    "\n",
    "print(f\"â­•Â missing entities in lookup : {missing_ent}\")\n",
    "print(f\"â­•Â missing relations in lookup: {missing_rel}\")\n",
    "print(f\"nodes collected : {len(node_tmp):,}\")\n",
    "print(f\"edges collected : {len(edge_tmp):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1cbd9_XX_PMA",
    "outputId": "f69289db-0483-40d0-c4a4-bfda657bf6a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "add nodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 557821/557821 [00:03<00:00, 165578.90it/s]\n",
      "add edges: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1075644/1075644 [00:09<00:00, 112930.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ”ï¸ graph: nodes=557,821  |  edges=1,075,644\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "import networkx as nx\n",
    "\n",
    "KG = nx.MultiDiGraph()\n",
    "\n",
    "for idx, info in tqdm.tqdm(node_tmp.items(), desc=\"add nodes\"):\n",
    "    KG.add_node(\n",
    "        idx,\n",
    "        label      = ordered_entities[idx],            \n",
    "        emb_idx    = idx,\n",
    "        types      = sorted(info.types) if info.types else None,\n",
    "        chunk_ids  = sorted(info.chunk_ids)               \n",
    "    )\n",
    "\n",
    "for (h, r, t), cid_set in tqdm.tqdm(edge_tmp.items(), desc=\"add edges\"):\n",
    "    KG.add_edge(\n",
    "        h, t, key=r,\n",
    "        relation  = ordered_relations[r],\n",
    "        emb_idx   = r,\n",
    "        chunk_ids = sorted(cid_set),\n",
    "    )\n",
    "\n",
    "print(f\"âœ”ï¸ graph: nodes={KG.number_of_nodes():,}  |  edges={KG.number_of_edges():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tw15ZNZk_Qza",
    "outputId": "e6efdb9b-f63b-4c7c-d377-bf553ebbc2df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ nodes with empty dict fixed â†’ 0\n"
     ]
    }
   ],
   "source": [
    "#9\n",
    "missing_attr = [n for n, d in KG.nodes(data=True) if not d]   # dict Ø®Ø§Ù„ÛŒ\n",
    "for n in missing_attr:\n",
    "    KG.nodes[n].update(\n",
    "        label     = ordered_entities[n],\n",
    "        emb_idx   = n,\n",
    "        types     = None,\n",
    "        chunk_ids = [],\n",
    "    )\n",
    "print(\"âš ï¸ nodes with empty dict fixed â†’\", len(missing_attr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46hO8OWZ_SFr",
    "outputId": "db5a0072-c929-41e0-c274-42f4d23889b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ saved â†’ /content/drive/MyDrive/HotpotQA_snapshot/hotpotqa_kg_v2.gpickle   |   33.9s\n",
      "size â‰ˆ 36.8â€¯MB\n"
     ]
    }
   ],
   "source": [
    "#10\n",
    "import gzip, pickle, time, os\n",
    "\n",
    "t0 = time.time()\n",
    "with gzip.open(GRAPH_PKL_PATH, \"wb\") as f:\n",
    "    pickle.dump(KG, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"ğŸ’¾ saved â†’ {GRAPH_PKL_PATH}   |   {(time.time()-t0):.1f}s\")\n",
    "print(f\"size â‰ˆ {os.path.getsize(GRAPH_PKL_PATH)/1e6:.1f}â€¯MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u2X5BbEC_T_S",
    "outputId": "3528e842-b27f-48e1-eee8-3e7ef17dc191"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ² node id=428304 | total_degree=8\n",
      "{'chunk_ids': [12142,\n",
      "               22669,\n",
      "               22776,\n",
      "               24862,\n",
      "               24863,\n",
      "               32979,\n",
      "               37351,\n",
      "               45197,\n",
      "               57423,\n",
      "               57424],\n",
      " 'emb_idx': 428304,\n",
      " 'label': 'silesia',\n",
      " 'types': ['GPE', 'LOC']}\n",
      "\n",
      "ğŸ”— edges involving this node:\n",
      "  out-edges: 2\n",
      "    silesia  --located_in-->  oderâ€“neisse line  | chunks=1\n",
      "    silesia  --present_day-->  wrocÅ‚aw  | chunks=1\n",
      "  in -edges: 6\n",
      "    maria szraiber  --born_in-->  silesia  | chunks=1\n",
      "    german  --displaced-->  silesia  | chunks=1\n",
      "    unrest  --expanded_into-->  silesia  | chunks=1\n",
      "    luther  --spoke_out_against-->  silesia  | chunks=1\n",
      "    frank  --located_in-->  silesia  | chunks=1\n",
      "    breslau  --located_in-->  silesia  | chunks=2\n"
     ]
    }
   ],
   "source": [
    "#11\n",
    "import random, pprint\n",
    "\n",
    "def sample_node(min_edges: int = 5, max_trials: int = 1000):\n",
    "    for _ in range(max_trials):\n",
    "        n = random.choice(list(KG.nodes))\n",
    "        deg = KG.out_degree(n) + KG.in_degree(n)\n",
    "        if deg >= min_edges:\n",
    "            return n, deg\n",
    "    raise RuntimeError(\"no node with required degree found\")\n",
    "\n",
    "nid, deg = sample_node()\n",
    "print(f\"ğŸ² node id={nid} | total_degree={deg}\")\n",
    "pprint.pprint(KG.nodes[nid])\n",
    "\n",
    "print(\"\\nğŸ”— edges involving this node:\")\n",
    "for direction, edges in ((\"out\", KG.out_edges(nid, keys=True, data=True)),\n",
    "                         (\"in \", KG.in_edges(nid,  keys=True, data=True))):\n",
    "    print(f\"  {direction}-edges: {len(edges)}\")\n",
    "    for h, t, k, data in itertools.islice(edges, 0, 10):   \n",
    "        print(f\"    {ordered_entities[h]}  --{ordered_relations[k]}-->  {ordered_entities[t]}  | chunks={len(data['chunk_ids'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gMEqGB8D_UiT",
    "outputId": "bb494449-ed4b-4deb-a7a6-43e0b4e827f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ² random edge\n",
      "independencia  --border_with-->  the los olivos district\n",
      "{'chunk_ids': [60838], 'emb_idx': 13974, 'relation': 'border_with'}\n"
     ]
    }
   ],
   "source": [
    "#12\n",
    "def sample_edge(max_trials: int = 1000):\n",
    "    for _ in range(max_trials):\n",
    "        h, t, k = random.choice(list(KG.edges(keys=True)))\n",
    "        data    = KG.get_edge_data(h, t, k)\n",
    "        if data: return h, t, k, data\n",
    "    raise RuntimeError(\"no edge with data found\")\n",
    "\n",
    "h, t, k, edata = sample_edge()\n",
    "print(\"ğŸ² random edge\")\n",
    "print(f\"{ordered_entities[h]}  --{edata['relation']}-->  {ordered_entities[t]}\")\n",
    "pprint.pprint(edata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tu6kciOjIWny",
    "outputId": "8db591e6-184b-4561-92a8-a67df725bbbd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "analyse missing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66237/66237 [00:06<00:00, 10181.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”  Unique missing entities: 1\n",
      "Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§:\n",
      " â€¢ ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unique_missing = set()\n",
    "\n",
    "for ch in tqdm.tqdm(chunks, desc=\"analyse missing\"):\n",
    "    for rel_raw in ch[\"relations\"]:\n",
    "        if rel_raw.count(\"->\") < 2:\n",
    "            continue\n",
    "        parts = [p.strip() for p in rel_raw.split(\"->\") if p.strip()]\n",
    "        heads, tails = parts[0], parts[-1]\n",
    "\n",
    "        for raw_side in (heads, tails):\n",
    "            norm_side = normalize_ent(raw_side)\n",
    "            if norm_side not in entity2idx:\n",
    "                unique_missing.add(norm_side)\n",
    "\n",
    "print(\"ğŸ”  Unique missing entities:\", len(unique_missing))   # Ø¨Ø§ÛŒØ¯ 4 Ø¨Ø§Ø´Ø¯\n",
    "for e in list(unique_missing)[:10]:\n",
    "    print(\" â€¢\", repr(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xl3BaI55IYVO",
    "outputId": "d09c9e4a-38f9-432b-c352-050a5d28e0f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ú©Ù„ Ø±Ø´ØªÙ‡â€ŒÙ‡Ø§ÛŒ relation : 1235425\n",
      "ÛŒØ§Ù„Â Ø³Ø§Ø®ØªÙ‡Â Ø´Ø¯Ù‡Â |Â skippedÂ â‰ˆÂ 7.9%Â Ø§Ø² Ú©Ù„ strings\n"
     ]
    }
   ],
   "source": [
    "total_rel_strings = sum(len(ch[\"relations\"]) for ch in chunks)\n",
    "print(\"Ú©Ù„ Ø±Ø´ØªÙ‡â€ŒÙ‡Ø§ÛŒ relation :\", total_rel_strings)\n",
    "skipped  = missing_ent          \n",
    "kept     = len(edge_tmp)        \n",
    "print(f\"ÛŒØ§Ù„Â Ø³Ø§Ø®ØªÙ‡Â Ø´Ø¯Ù‡Â |Â skippedÂ â‰ˆÂ {skipped/total_rel_strings:.1%}Â Ø§Ø² Ú©Ù„ strings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iw8cyYx4StK6",
    "outputId": "1599010f-9c17-4f79-c31a-b61120c4c2a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes with empty or None types: 271238\n"
     ]
    }
   ],
   "source": [
    "empty_types_count = sum(\n",
    "    1\n",
    "    for _, data in KG.nodes(data=True)\n",
    "    if not data.get('types')  # types is None or empty list\n",
    ")\n",
    "print(f\"Nodes with empty or None types: {empty_types_count}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
