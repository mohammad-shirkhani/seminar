{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nrcae441PwLd"
   },
   "outputs": [],
   "source": [
    "# 1â€¯Â Install & import libraries\n",
    "!pip install -q google-genai tqdm\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from pathlib import Path\n",
    "import os, json, time, re, shutil, tqdm\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w5k1_JO9Py_f",
    "outputId": "da82f550-db9a-4235-989f-8b92e70b6679"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Gemini client initialised\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# 2â€¯Â Authenticate Gemini & mount Drive\n",
    "\n",
    "\n",
    "# === Provide your API key (or export it beforehand) ===\n",
    "os.environ['GEMINI_API_KEY'] = ''  \n",
    "client = genai.Client()\n",
    "print('Gemini client initialised')\n",
    "\n",
    "# Mount Drive (optional â€“ comment out if running locally)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h4rtemBwP5jN",
    "outputId": "f704da5e-5bcb-4b7b-dea7-02c403db9d6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Evidence files ready\n"
     ]
    }
   ],
   "source": [
    "# ## 3â€¯Â Copy evidence files into the working directory\n",
    "\n",
    "drive_root = Path('/content/drive/MyDrive/KGP_evidence')\n",
    "work_dir   = Path('/content/llm_answer_generation')\n",
    "work_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for filename in ['hotpotqa_evidence_qwen.json', '2wiki_evidence_qwen.json']:\n",
    "    shutil.copy2(drive_root/filename, work_dir/filename)\n",
    "print('ðŸ“„ Evidence files ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SAOGDA5LP-m9"
   },
   "outputs": [],
   "source": [
    "# ## 4â€¯Â Utility helpers\n",
    "\n",
    "\n",
    "def load_json(path: Path) -> List[Dict]:\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def save_json(obj, path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(obj, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "def _clean_answer(text: str) -> str:\n",
    "    \"\"\"Collapse whitespace & strip any <think>â€¦</think> blocks if they slipped through.\"\"\"\n",
    "    text = re.sub(r\"<think[^>]*?>.*?</think>\", \" \", text, flags=re.S)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "\n",
    "def split_thoughts_and_answer(parts) -> (str, str):\n",
    "    \"\"\"Separate thought summaries from the final answer.\"\"\"\n",
    "    thoughts, answer = [], []\n",
    "    for p in parts:\n",
    "        if not getattr(p, 'text', None):\n",
    "            continue\n",
    "        if getattr(p, 'thought', False):\n",
    "            thoughts.append(p.text)\n",
    "        else:\n",
    "            answer.append(p.text)\n",
    "    return \"\\n\".join(thoughts).strip(), \" \".join(answer).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_-2RAyL2QFLV"
   },
   "outputs": [],
   "source": [
    "# ## 5  Generation pipeline (thinking ON, thoughts stripped from `response`)\n",
    "\n",
    "def generate_answers(\n",
    "    evidence_file: Path,\n",
    "    save_file: Path,\n",
    "    *,\n",
    "    max_output_tokens: int = 128,\n",
    "    temperature: float = 0.6,\n",
    "    top_p: float = 0.95,\n",
    "    thinking_budget: int = -1,   # dynamic\n",
    "    pause_secs: float = 5.0,\n",
    "):\n",
    "    data = load_json(evidence_file)\n",
    "    responses = []\n",
    "\n",
    "    prompt_template = (\n",
    "        \"Given the question and its associated contexts below, please generate a concise, \"\n",
    "        \"precise answer in English. The answer must strictly adhere to the following guidelines:\\n\"\n",
    "        \"- The answer should be directly relevant to the question.\\n\"\n",
    "        \"- Provide the answer in a clear, straightforward format.\\n\"\n",
    "        \"- Limit your answer to no more than 10 words, focusing on the essential information requested.\\n\"\n",
    "        \"- If the provided contexts do not contain enough information but you know the answer, still provide it; \"\n",
    "        \"otherwise respond with \\\"Information not available\\\".\\n\"\n",
    "        \"- Do not include any additional tokens, explanations, or information beyond the direct answer.\\n\\n\"\n",
    "        \"QUESTION: {question}\\n\"\n",
    "        \"CONTEXT:\\n{context}\\n\"\n",
    "        \"ANSWER:\"\n",
    "    )\n",
    "\n",
    "    none_template = (\n",
    "        \"Given the following question, create a final answer in English to the question.\\n\"\n",
    "        \"QUESTION: {question}\\n\"\n",
    "        \"ANSWER:\"\n",
    "    )\n",
    "\n",
    "    for rec in tqdm.tqdm(data, total=len(data)):\n",
    "        ctx_list = rec['evidence']\n",
    "        if ctx_list:\n",
    "            ctx_block = \"\\n\".join(f\"{i+1}: {c}\" for i, c in enumerate(ctx_list))\n",
    "            user_prompt = prompt_template.format(question=rec['question'], context=ctx_block)\n",
    "        else:\n",
    "            user_prompt = none_template.format(question=rec['question'])\n",
    "\n",
    "        response = client.models.generate_content(\n",
    "            model='gemini-2.5-flash',\n",
    "            contents=user_prompt,\n",
    "            config=types.GenerateContentConfig(\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                max_output_tokens=max_output_tokens,\n",
    "                thinking_config=types.ThinkingConfig(\n",
    "                    thinking_budget=thinking_budget,\n",
    "                    include_thoughts=True\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            cand_container = response.candidates\n",
    "            cand = cand_container[0] if isinstance(cand_container, (list, tuple)) else cand_container\n",
    "            parts = cand.content.parts\n",
    "            thought_text, answer_text = split_thoughts_and_answer(parts)\n",
    "        except (TypeError, IndexError, AttributeError):\n",
    "            thought_text = \"\"\n",
    "            answer_text  = getattr(response, \"text\", \"Information not available\")\n",
    "\n",
    "        short_answer = _clean_answer(answer_text)\n",
    "\n",
    "        responses.append({\n",
    "            'type':     rec['type'],\n",
    "            'question': rec['question'],\n",
    "            'gt':       rec['answer'],\n",
    "            'thoughts': thought_text,\n",
    "            'answer':   answer_text,\n",
    "            'response': short_answer,\n",
    "        })\n",
    "\n",
    "        save_json(responses, save_file)  # checkpoint\n",
    "        time.sleep(pause_secs)           # respect 10 RPM\n",
    "\n",
    "    print(f\"Finished â€“ saved to {save_file}\")\n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ScAQfXotQRjQ",
    "outputId": "a63e4c36-f832-41d3-a6ae-08681f03b826"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [13:42<00:00,  8.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished â€“ saved to /content/drive/MyDrive/KGP_evidence/hotpotqa_answers_gemini.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 6â€¯Â Run generation on HotpotQA & 2Wiki\n",
    "\n",
    "# %%\n",
    "hotpot_out = drive_root / 'hotpotqa_answers_gemini.json'\n",
    "_ = generate_answers(\n",
    "        work_dir / 'hotpotqa_evidence_qwen.json',\n",
    "        hotpot_out,\n",
    "        max_output_tokens=1024,\n",
    "        temperature=0.6,\n",
    "        top_p=0.95,\n",
    "        thinking_budget=-1,  # dynamic\n",
    "        pause_secs=5.0,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
